"""
Synthesis Agent for LangGraph workflow.

This agent integrates the sophisticated synthesis logic from the original
SynthesisTool, handling the combination of multiple sub-answers into a
comprehensive, well-structured, and cited final response.
"""

import os
import json
from typing import Dict, Any, List

# Add parent directories to path for imports
from .base_agent import BaseLangGraphAgent
from state import AgentState

# Import the original synthesis tool logic
from tools.synthesis_tool import SynthesisTool

class SynthesisAgent(BaseLangGraphAgent):
    """
    Synthesis Agent for comprehensive final answer generation.
    
    This agent leverages the existing SynthesisTool implementation while
    adapting it to the LangGraph workflow. It handles:
    - Combining multiple sub-query answers
    - Creating well-structured responses
    - Proper citation and source attribution
    - Quality assessment and confidence scoring
    """
    
    def __init__(self):
        """Initialize the Synthesis Agent with Tier 1 model for high-quality synthesis."""
        super().__init__(model_tier="tier_1", agent_name="SynthesisAgent")
        
        # Initialize the original synthesis tool
        self.synthesis_tool = SynthesisTool()
        
        self.logger.info("Synthesis Agent initialized successfully")
    
    async def execute(self, state: AgentState) -> Dict[str, Any]:
        """
        Execute the synthesis logic using the original SynthesisTool.
        
        Args:
            state: Current workflow state
            
        Returns:
            Dictionary containing synthesis results
        """
        user_query = state["user_query"]
        # Use the original_query if available, otherwise fall back to the current user_query.
        # This is crucial for contextual understanding in conversational follow-ups.
        original_query = state.get("original_query", user_query)
        sub_query_answers = state.get("sub_query_answers", [])
        
        if not sub_query_answers:
            self.logger.error("No sub-query answers provided to synthesis agent")
            return {
                "error_state": {
                    "agent": self.agent_name,
                    "error_type": "MissingSubAnswers",
                    "error_message": "No sub-query answers provided for synthesis",
                    "timestamp": "now"
                },
                "current_step": "error",
                "workflow_status": "failed"
            }
        
        self.logger.info(f"Synthesizing final answer from {len(sub_query_answers)} sub-answers for original query: '{original_query[:100]}'")
        
        try:
            # Execute the original synthesis tool, passing the original query for context
            synthesis_result = await self._execute_synthesis(original_query, user_query, sub_query_answers)
            
            # Process and format the results for LangGraph state
            return await self._process_synthesis_result(synthesis_result, state)
            
        except Exception as e:
            self.logger.error(f"Error in synthesis execution: {e}")
            return {
                "error_state": {
                    "agent": self.agent_name,
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "timestamp": "now"
                },
                "current_step": "error",
                "workflow_status": "failed"
            }
    
    async def _execute_synthesis(self, original_query: str, current_query: str, sub_query_answers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Executes the synthesis using the original tool logic.
        
        Args:
            original_query: The query that initiated the research.
            current_query: The most recent user query (could be a follow-up).
            sub_query_answers: List of sub-query answers to synthesize
            
        Returns:
            Synthesis results from the synthesis tool
        """
        # Execute the original synthesis tool
        synthesis_result = self.synthesis_tool(
            original_query=original_query,
            current_query=current_query,
            sub_answers=sub_query_answers
        )
        
        return synthesis_result
    
    async def _process_synthesis_result(self, synthesis_result: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """
        Processes the result from the synthesis tool and formats for LangGraph.
        
        Args:
            synthesis_result: Result from the synthesis tool
            state: Current workflow state
            
        Returns:
            Formatted synthesis output for state update
        """
        final_answer = synthesis_result.get("final_answer", "")
        
        if not final_answer:
            self.logger.warning("No final answer generated by synthesis tool")
            final_answer = "I was unable to generate a comprehensive answer based on the available information."
        
        # Extract citations and sources
        source_citations = self._extract_citations(final_answer, state.get("sub_query_answers", []))
        
        # Calculate synthesis quality metrics
        synthesis_metadata = self._calculate_synthesis_metadata(final_answer, state.get("sub_query_answers", []))
        
        # Calculate confidence score
        confidence_score = self._calculate_confidence_score(synthesis_metadata, state)
        
        self.logger.info(f"Synthesis completed: {len(final_answer)} characters, confidence: {confidence_score:.2f}")
        
        return {
            "final_answer": final_answer,
            "synthesis_metadata": synthesis_metadata,
            "source_citations": source_citations,
            "confidence_score": confidence_score,
            "current_step": "memory_update",
            "workflow_status": "running"
        }
    
    def _extract_citations(self, final_answer: str, sub_query_answers: List[Dict[str, Any]]) -> List[str]:
        """
        Extracts and validates source citations from the final answer.
        
        Args:
            final_answer: The synthesized final answer
            sub_query_answers: Original sub-query answers
            
        Returns:
            List of unique source citations
        """
        citations = []
        
        # Extract sources from sub-query answers
        for sub_answer in sub_query_answers:
            sources = sub_answer.get("sources_used", [])
            citations.extend(sources)
        
        # Remove duplicates while preserving order
        unique_citations = []
        seen = set()
        for citation in citations:
            if citation not in seen:
                unique_citations.append(citation)
                seen.add(citation)
        
        return unique_citations
    
    def _calculate_synthesis_metadata(self, final_answer: str, sub_query_answers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Calculates comprehensive metadata for the synthesis process.
        
        Args:
            final_answer: The synthesized final answer
            sub_query_answers: Original sub-query answers
            
        Returns:
            Synthesis metadata dictionary
        """
        # Basic metrics
        answer_length = len(final_answer)
        word_count = len(final_answer.split())
        sub_answer_count = len(sub_query_answers)
        
        # Source integration metrics
        total_sources = sum(len(ans.get("sources_used", [])) for ans in sub_query_answers)
        unique_sources = len(set().union(*[ans.get("sources_used", []) for ans in sub_query_answers]))
        
        # Content integration metrics
        successful_integrations = len([ans for ans in sub_query_answers if ans.get("answer") and len(ans["answer"]) > 50])
        
        # Quality indicators
        has_citations = "[Source:" in final_answer or "Section" in final_answer
        has_structure = any(marker in final_answer for marker in ["1.", "2.", "â€¢", "-", "**"])
        
        return {
            "answer_length_chars": answer_length,
            "answer_word_count": word_count,
            "sub_answers_integrated": sub_answer_count,
            "successful_integrations": successful_integrations,
            "total_sources_referenced": total_sources,
            "unique_sources_used": unique_sources,
            "has_proper_citations": has_citations,
            "has_structured_format": has_structure,
            "integration_rate": successful_integrations / sub_answer_count if sub_answer_count > 0 else 0.0,
            "source_diversity": unique_sources / total_sources if total_sources > 0 else 0.0
        }
    
    def _calculate_confidence_score(self, synthesis_metadata: Dict[str, Any], state: AgentState) -> float:
        """
        Calculates an overall confidence score for the synthesis.
        
        Args:
            synthesis_metadata: Metadata from synthesis process
            state: Current workflow state
            
        Returns:
            Confidence score between 0.0 and 1.0
        """
        # For now, we use a simple heuristic based on the number of sources
        # and whether any fallback methods were required.
        
        num_sources = len(synthesis_metadata.get("sources_used", []))
        
        # Start with a base score
        score = 0.5
        
        # Increase score for more sources
        if num_sources > 0:
            score += 0.1
        if num_sources > 2:
            score += 0.1
        if num_sources > 4:
            score += 0.1
        
        # Simple confidence calculation
        return min(1.0, score) # Cap at 1.0
    
    def _validate_agent_specific_state(self, state: AgentState) -> None:
        """
        Validates synthesis agent specific state requirements.
        
        Args:
            state: State to validate
            
        Raises:
            ValueError: If required fields are missing
        """
        if not state.get("sub_query_answers"):
            raise ValueError("sub_query_answers is required for synthesis agent")
        
        if state.get("current_step") not in ["synthesis", "research"]:
            self.logger.warning(f"Unexpected current_step '{state.get('current_step')}' for synthesis agent")
    
    def _apply_agent_specific_updates(self, state: AgentState, output_data: Dict[str, Any]) -> AgentState:
        """
        Applies synthesis-specific state updates.
        
        Args:
            state: Current state
            output_data: Synthesis output data
            
        Returns:
            Updated state
        """
        updated_state = state.copy()
        
        # Store synthesis metadata for debugging
        if updated_state.get("intermediate_outputs") is not None:
            synthesis_metadata = output_data.get("synthesis_metadata", {})
            
            updated_state["intermediate_outputs"]["synthesis_details"] = {
                "answer_length": synthesis_metadata.get("answer_length_chars", 0),
                "word_count": synthesis_metadata.get("answer_word_count", 0),
                "sources_integrated": synthesis_metadata.get("unique_sources_used", 0),
                "confidence_score": output_data.get("confidence_score", 0.0),
                "has_citations": synthesis_metadata.get("has_proper_citations", False)
            }
        
        # Update quality metrics
        if updated_state.get("quality_metrics") is not None:
            synthesis_metadata = output_data.get("synthesis_metadata", {})
            updated_state["quality_metrics"]["synthesis_quality_score"] = output_data.get("confidence_score", 0.0)
            updated_state["quality_metrics"]["context_sufficiency_score"] = synthesis_metadata.get("integration_rate", 0.0)
        
        # Set end time for workflow
        updated_state["end_time"] = state.get("start_time", "")  # This should be current time in real implementation
        
        return updated_state

class EnhancedSynthesisAgent(SynthesisAgent):
    """
    Enhanced Synthesis Agent with specialized strategies for different
    query types (calculation, comparison, compliance).
    """
    
    def __init__(self):
        super().__init__()
        self.logger.info("Enhanced Synthesis Agent initialized with specialized strategies")

    async def execute(self, state: AgentState) -> Dict[str, Any]:
        """
        Executes the synthesis logic, trying enhanced strategies first
        and falling back to the standard synthesis if needed.
        """
        # Try enhanced synthesis first
        enhanced_result = await self._try_enhanced_synthesis(state)
        
        if enhanced_result:
            return await self._process_synthesis_result(enhanced_result, state)
        
        # Fallback to standard synthesis
        self.logger.warning("No enhanced synthesis strategy applied. Falling back to standard synthesis.")
        return await super().execute(state)

    async def _try_enhanced_synthesis(self, state: AgentState) -> Dict[str, Any]:
        """Tries to apply an enhanced synthesis strategy based on the state."""
        if state.get("math_calculation_needed"):
            self.logger.info("Applying 'Calculation' synthesis strategy.")
            return await self._enhance_calculation_synthesis(state)
        
        # Other strategies can be added here
        # elif state.get("is_comparison_query"):
        #     return await self._enhance_comparison_synthesis(state)
        
        return None

    async def _enhance_calculation_synthesis(self, state: AgentState) -> Dict[str, Any]:
        """
        Enhanced synthesis for queries requiring calculations.
        This prompt instructs the LLM to perform the math.
        """
        user_query = state["user_query"]
        sub_query_answers = state.get("sub_query_answers", [])

        # Construct the sub-answers string separately to avoid f-string syntax limitations.
        sub_answers_text = "".join([
            f"Sub-Query: {ans.get('sub_query', 'N/A')}\nAnswer: {ans.get('answer', 'N/A')}\n\n"
            for ans in sub_query_answers
        ])

        prompt = f"""
        You are a Virginia Building Code expert and a skilled technical analyst.
        Your task is to provide a comprehensive answer to the user's query, which requires performing mathematical calculations.

        **USER QUERY:**
        {user_query}

        **RESEARCHED CONTEXT & SUB-ANSWERS:**
        ---
        {sub_answers_text}
        ---

        **INSTRUCTIONS:**
        1.  **Synthesize and Calculate**: Review all the provided context to understand the problem.
        2.  **Identify Formulas**: Extract the necessary formulas and variables from the context.
        3.  **Perform the Calculation**: Using the data and formulas, perform the mathematical calculations step-by-step. Show your work clearly.
        4.  **State Assumptions**: If any values are not explicitly provided, state reasonable assumptions (e.g., "Assuming a standard tributary area of X...").
        5.  **Provide the Final Answer**: Give a clear, numerical answer to the user's question based on your calculation.
        6.  **Cite Sources**: Reference the relevant building code sections (e.g., "According to Section 1607.12...").
        7.  **Do Not Hedge**: Provide a confident, definitive answer. Do not say you cannot perform calculations. You are the expert.

        **FINAL ANSWER FORMAT:**
        - Start with a clear statement of the final calculated value.
        - Provide a "Methodology" section explaining how you arrived at the answer.
        - Show the formula used, the values substituted, and the step-by-step calculation.
        - Conclude with any necessary context or explanations based on the code.

        **Your Comprehensive Answer:**
        """
        
        response_text = await self.generate_content_async(prompt)
        return {"final_answer": response_text}

    async def _enhance_comparison_synthesis(self, state: AgentState) -> Dict[str, Any]:
        """Enhanced synthesis for comparison queries."""
        result = await super().execute(state)
        
        if result.get("final_answer"):
            enhanced_answer = self._format_comparison_answer(result["final_answer"])
            result["final_answer"] = enhanced_answer
            result["synthesis_metadata"]["enhanced_for"] = "comparison"
        
        return result
    
    def _format_comparison_answer(self, answer: str) -> str:
        """Formats answer for comparison queries with clear contrast structure."""
        return f"**Comparative Analysis:**\n\n{answer}\n\n*Note: This comparison is based on the Virginia Building Code requirements.*"
    
    async def _enhance_compliance_synthesis(self, state: AgentState) -> Dict[str, Any]:
        """Enhances synthesis for compliance queries."""
        result = await super().execute(state)
        
        if result.get("final_answer"):
            enhanced_answer = self._format_compliance_answer(result["final_answer"])
            result["final_answer"] = enhanced_answer
            result["synthesis_metadata"]["enhanced_for"] = "compliance"
        
        return result
    
    def _format_compliance_answer(self, answer: str) -> str:
        """Formats answer for compliance queries with clear regulatory structure."""
        return f"**Compliance Requirements:**\n\n{answer}\n\n*Note: Compliance must be verified with local building officials and current code editions.*"
    
    async def _enhance_procedural_synthesis(self, state: AgentState) -> Dict[str, Any]:
        """Enhances synthesis for procedural queries."""
        result = await super().execute(state)
        
        if result.get("final_answer"):
            enhanced_answer = self._format_procedural_answer(result["final_answer"])
            result["final_answer"] = enhanced_answer
            result["synthesis_metadata"]["enhanced_for"] = "procedural"
        
        return result
    
    def _format_procedural_answer(self, answer: str) -> str:
        """Formats answer for procedural queries with clear step-by-step structure."""
        return f"**Step-by-Step Procedure:**\n\n{answer}\n\n*Note: Follow all steps in sequence and verify compliance with local building requirements.*"