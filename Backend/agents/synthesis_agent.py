"""
Synthesis Agent for LangGraph workflow.

This agent integrates the sophisticated synthesis logic from the original
SynthesisTool, handling the combination of multiple sub-answers into a
comprehensive, well-structured, and cited final response.
"""

import os
import json
from typing import Dict, Any, List

# Add parent directories to path for imports
from .base_agent import BaseLangGraphAgent
from state import AgentState

# Import the original synthesis tool logic
from tools.synthesis_tool import SynthesisTool

class SynthesisAgent(BaseLangGraphAgent):
    """
    Synthesis Agent for comprehensive final answer generation.
    
    This agent leverages the existing SynthesisTool implementation while
    adapting it to the LangGraph workflow. It handles:
    - Combining multiple sub-query answers
    - Creating well-structured responses
    - Proper citation and source attribution
    - Quality assessment and confidence scoring
    """
    
    def __init__(self):
        """Initialize the Synthesis Agent with Tier 1 model for high-quality synthesis."""
        super().__init__(model_tier="tier_1", agent_name="SynthesisAgent")
        
        # Initialize the original synthesis tool
        self.synthesis_tool = SynthesisTool()
        
        self.logger.info("Synthesis Agent initialized successfully")
    
    async def execute(self, state: AgentState) -> Dict[str, Any]:
        """
        Execute the synthesis logic using the original SynthesisTool.
        
        Args:
            state: Current workflow state
            
        Returns:
            Dictionary containing synthesis results
        """
        user_query = state["user_query"]
        sub_query_answers = state.get("sub_query_answers", [])
        
        if not sub_query_answers:
            self.logger.error("No sub-query answers provided to synthesis agent")
            return {
                "error_state": {
                    "agent": self.agent_name,
                    "error_type": "MissingSubAnswers",
                    "error_message": "No sub-query answers provided for synthesis",
                    "timestamp": "now"
                },
                "current_step": "error",
                "workflow_status": "failed"
            }
        
        self.logger.info(f"Synthesizing final answer from {len(sub_query_answers)} sub-answers")
        
        try:
            # Execute the original synthesis tool
            synthesis_result = await self._execute_synthesis(user_query, sub_query_answers)
            
            # Process and format the results for LangGraph state
            return await self._process_synthesis_result(synthesis_result, state)
            
        except Exception as e:
            self.logger.error(f"Error in synthesis execution: {e}")
            return {
                "error_state": {
                    "agent": self.agent_name,
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "timestamp": "now"
                },
                "current_step": "error",
                "workflow_status": "failed"
            }
    
    async def _execute_synthesis(self, user_query: str, sub_query_answers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Executes the synthesis using the original tool logic.
        
        Args:
            user_query: Original user question
            sub_query_answers: List of sub-query answers to synthesize
            
        Returns:
            Synthesis results from the synthesis tool
        """
        # Execute the original synthesis tool
        synthesis_result = self.synthesis_tool(
            query=user_query,
            sub_answers=sub_query_answers
        )
        
        return synthesis_result
    
    async def _process_synthesis_result(self, synthesis_result: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """
        Processes the result from the synthesis tool and formats for LangGraph.
        
        Args:
            synthesis_result: Result from the synthesis tool
            state: Current workflow state
            
        Returns:
            Formatted synthesis output for state update
        """
        final_answer = synthesis_result.get("final_answer", "")
        
        if not final_answer:
            self.logger.warning("No final answer generated by synthesis tool")
            final_answer = "I was unable to generate a comprehensive answer based on the available information."
        
        # Extract citations and sources
        source_citations = self._extract_citations(final_answer, state.get("sub_query_answers", []))
        
        # Calculate synthesis quality metrics
        synthesis_metadata = self._calculate_synthesis_metadata(final_answer, state.get("sub_query_answers", []))
        
        # Calculate confidence score
        confidence_score = self._calculate_confidence_score(synthesis_metadata, state)
        
        self.logger.info(f"Synthesis completed: {len(final_answer)} characters, confidence: {confidence_score:.2f}")
        
        return {
            "final_answer": final_answer,
            "synthesis_metadata": synthesis_metadata,
            "source_citations": source_citations,
            "confidence_score": confidence_score,
            "current_step": "memory_update",
            "workflow_status": "running"
        }
    
    def _extract_citations(self, final_answer: str, sub_query_answers: List[Dict[str, Any]]) -> List[str]:
        """
        Extracts and validates source citations from the final answer.
        
        Args:
            final_answer: The synthesized final answer
            sub_query_answers: Original sub-query answers
            
        Returns:
            List of unique source citations
        """
        citations = []
        
        # Extract sources from sub-query answers
        for sub_answer in sub_query_answers:
            sources = sub_answer.get("sources_used", [])
            citations.extend(sources)
        
        # Remove duplicates while preserving order
        unique_citations = []
        seen = set()
        for citation in citations:
            if citation not in seen:
                unique_citations.append(citation)
                seen.add(citation)
        
        return unique_citations
    
    def _calculate_synthesis_metadata(self, final_answer: str, sub_query_answers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Calculates comprehensive metadata for the synthesis process.
        
        Args:
            final_answer: The synthesized final answer
            sub_query_answers: Original sub-query answers
            
        Returns:
            Synthesis metadata dictionary
        """
        # Basic metrics
        answer_length = len(final_answer)
        word_count = len(final_answer.split())
        sub_answer_count = len(sub_query_answers)
        
        # Source integration metrics
        total_sources = sum(len(ans.get("sources_used", [])) for ans in sub_query_answers)
        unique_sources = len(set().union(*[ans.get("sources_used", []) for ans in sub_query_answers]))
        
        # Content integration metrics
        successful_integrations = len([ans for ans in sub_query_answers if ans.get("answer") and len(ans["answer"]) > 50])
        
        # Quality indicators
        has_citations = "[Source:" in final_answer or "Section" in final_answer
        has_structure = any(marker in final_answer for marker in ["1.", "2.", "â€¢", "-", "**"])
        
        return {
            "answer_length_chars": answer_length,
            "answer_word_count": word_count,
            "sub_answers_integrated": sub_answer_count,
            "successful_integrations": successful_integrations,
            "total_sources_referenced": total_sources,
            "unique_sources_used": unique_sources,
            "has_proper_citations": has_citations,
            "has_structured_format": has_structure,
            "integration_rate": successful_integrations / sub_answer_count if sub_answer_count > 0 else 0.0,
            "source_diversity": unique_sources / total_sources if total_sources > 0 else 0.0
        }
    
    def _calculate_confidence_score(self, synthesis_metadata: Dict[str, Any], state: AgentState) -> float:
        """
        Calculates an overall confidence score for the synthesis.
        
        Args:
            synthesis_metadata: Metadata from synthesis process
            state: Current workflow state
            
        Returns:
            Confidence score between 0.0 and 1.0
        """
        # Base score from integration success
        integration_score = synthesis_metadata.get("integration_rate", 0.0)
        
        # Source quality score
        source_score = min(synthesis_metadata.get("unique_sources_used", 0) / 3.0, 1.0)
        
        # Content quality indicators
        citation_bonus = 0.1 if synthesis_metadata.get("has_proper_citations", False) else 0.0
        structure_bonus = 0.1 if synthesis_metadata.get("has_structured_format", False) else 0.0
        
        # Length appropriateness (not too short, not excessively long)
        length_score = min(synthesis_metadata.get("answer_word_count", 0) / 200.0, 1.0)
        if length_score < 0.2:  # Penalize very short answers
            length_score *= 0.5
        
        # Research quality from previous steps
        research_quality = state.get("retrieval_quality_scores", {}).get("overall_quality", 0.7)
        
        # Combined confidence score
        confidence = (
            integration_score * 0.3 +
            source_score * 0.2 +
            length_score * 0.2 +
            research_quality * 0.2 +
            citation_bonus +
            structure_bonus
        )
        
        return min(max(confidence, 0.0), 1.0)  # Clamp between 0 and 1
    
    def _validate_agent_specific_state(self, state: AgentState) -> None:
        """
        Validates synthesis agent specific state requirements.
        
        Args:
            state: State to validate
            
        Raises:
            ValueError: If required fields are missing
        """
        if not state.get("sub_query_answers"):
            raise ValueError("sub_query_answers is required for synthesis agent")
        
        if state.get("current_step") not in ["synthesis", "research"]:
            self.logger.warning(f"Unexpected current_step '{state.get('current_step')}' for synthesis agent")
    
    def _apply_agent_specific_updates(self, state: AgentState, output_data: Dict[str, Any]) -> AgentState:
        """
        Applies synthesis-specific state updates.
        
        Args:
            state: Current state
            output_data: Synthesis output data
            
        Returns:
            Updated state
        """
        updated_state = state.copy()
        
        # Store synthesis metadata for debugging
        if updated_state.get("intermediate_outputs") is not None:
            synthesis_metadata = output_data.get("synthesis_metadata", {})
            
            updated_state["intermediate_outputs"]["synthesis_details"] = {
                "answer_length": synthesis_metadata.get("answer_length_chars", 0),
                "word_count": synthesis_metadata.get("answer_word_count", 0),
                "sources_integrated": synthesis_metadata.get("unique_sources_used", 0),
                "confidence_score": output_data.get("confidence_score", 0.0),
                "has_citations": synthesis_metadata.get("has_proper_citations", False)
            }
        
        # Update quality metrics
        if updated_state.get("quality_metrics") is not None:
            synthesis_metadata = output_data.get("synthesis_metadata", {})
            updated_state["quality_metrics"]["synthesis_quality_score"] = output_data.get("confidence_score", 0.0)
            updated_state["quality_metrics"]["context_sufficiency_score"] = synthesis_metadata.get("integration_rate", 0.0)
        
        # Set end time for workflow
        updated_state["end_time"] = state.get("start_time", "")  # This should be current time in real implementation
        
        return updated_state

class EnhancedSynthesisAgent(SynthesisAgent):
    """
    Enhanced Synthesis Agent with additional LangGraph-specific optimizations.
    
    This version includes extra features for the LangGraph workflow while
    maintaining full compatibility with the original synthesis logic.
    """
    
    def __init__(self):
        """Initialize the Enhanced Synthesis Agent."""
        super().__init__()
        self.agent_name = "EnhancedSynthesisAgent"
        
        # Enhanced synthesis strategies
        self.synthesis_strategies = {
            "technical_calculation": self._enhance_calculation_synthesis,
            "comparative_analysis": self._enhance_comparison_synthesis,
            "regulatory_compliance": self._enhance_compliance_synthesis,
            "procedural_guidance": self._enhance_procedural_synthesis
        }
    
    async def execute(self, state: AgentState) -> Dict[str, Any]:
        """
        Enhanced execution with strategy-based synthesis optimization.
        
        Args:
            state: Current workflow state
            
        Returns:
            Dictionary containing enhanced synthesis results
        """
        # First apply enhanced synthesis if applicable
        enhanced_result = await self._try_enhanced_synthesis(state)
        if enhanced_result:
            return enhanced_result
        
        # Fall back to original synthesis logic
        return await super().execute(state)
    
    async def _try_enhanced_synthesis(self, state: AgentState) -> Dict[str, Any]:
        """
        Tries enhanced synthesis strategies for specific query types.
        
        Args:
            state: Current workflow state
            
        Returns:
            Enhanced synthesis result if applicable, None otherwise
        """
        user_query = state["user_query"].lower()
        
        # Check for calculation queries
        if any(word in user_query for word in ["calculate", "formula", "equation", "compute"]):
            return await self._enhance_calculation_synthesis(state)
        
        # Check for comparison queries
        if any(word in user_query for word in ["compare", "difference", "versus", "vs"]):
            return await self._enhance_comparison_synthesis(state)
        
        # Check for compliance queries
        if any(word in user_query for word in ["comply", "compliance", "requirement", "shall"]):
            return await self._enhance_compliance_synthesis(state)
        
        return None
    
    async def _enhance_calculation_synthesis(self, state: AgentState) -> Dict[str, Any]:
        """Enhances synthesis for calculation-focused queries."""
        # Use original synthesis but with enhanced formatting prompts
        result = await super().execute(state)
        
        # Post-process to ensure calculation format
        if result.get("final_answer"):
            enhanced_answer = self._format_calculation_answer(result["final_answer"])
            result["final_answer"] = enhanced_answer
            result["synthesis_metadata"]["enhanced_for"] = "calculation"
        
        return result
    
    async def _enhance_comparison_synthesis(self, state: AgentState) -> Dict[str, Any]:
        """Enhances synthesis for comparison queries."""
        result = await super().execute(state)
        
        if result.get("final_answer"):
            enhanced_answer = self._format_comparison_answer(result["final_answer"])
            result["final_answer"] = enhanced_answer
            result["synthesis_metadata"]["enhanced_for"] = "comparison"
        
        return result
    
    async def _enhance_compliance_synthesis(self, state: AgentState) -> Dict[str, Any]:
        """Enhances synthesis for compliance queries."""
        result = await super().execute(state)
        
        if result.get("final_answer"):
            enhanced_answer = self._format_compliance_answer(result["final_answer"])
            result["final_answer"] = enhanced_answer
            result["synthesis_metadata"]["enhanced_for"] = "compliance"
        
        return result
    
    def _format_calculation_answer(self, answer: str) -> str:
        """Formats answer for calculation queries with clear formula presentation."""
        if "=" in answer and any(char.isdigit() for char in answer):
            # Already has calculation format
            return answer
        
        # Add calculation formatting hints
        return f"**Calculation Response:**\n\n{answer}\n\n*Note: Please verify all calculations with the current Virginia Building Code.*"
    
    def _format_comparison_answer(self, answer: str) -> str:
        """Formats answer for comparison queries with clear contrast structure."""
        return f"**Comparative Analysis:**\n\n{answer}\n\n*Note: This comparison is based on the Virginia Building Code requirements.*"
    
    def _format_compliance_answer(self, answer: str) -> str:
        """Formats answer for compliance queries with clear regulatory structure."""
        return f"**Compliance Requirements:**\n\n{answer}\n\n*Note: Compliance must be verified with local building officials and current code editions.*"
    
    async def _enhance_procedural_synthesis(self, state: AgentState) -> Dict[str, Any]:
        """Enhances synthesis for procedural queries."""
        result = await super().execute(state)
        
        if result.get("final_answer"):
            enhanced_answer = self._format_procedural_answer(result["final_answer"])
            result["final_answer"] = enhanced_answer
            result["synthesis_metadata"]["enhanced_for"] = "procedural"
        
        return result
    
    def _format_procedural_answer(self, answer: str) -> str:
        """Formats answer for procedural queries with clear step-by-step structure."""
        return f"**Step-by-Step Procedure:**\n\n{answer}\n\n*Note: Follow all steps in sequence and verify compliance with local building requirements.*"