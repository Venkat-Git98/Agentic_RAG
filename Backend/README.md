# ğŸ¤– LangGraph Agentic AI Backend

A sophisticated multi-agent AI system built on LangGraph that orchestrates specialized agents to provide intelligent, context-aware responses through advanced reasoning and knowledge retrieval capabilities.

## ğŸŒŸ Overview

This backend implements a state-of-the-art agentic AI architecture where multiple specialized agents collaborate through a carefully orchestrated workflow. Each agent has a specific role in the cognitive pipeline, from initial query triage to complex research orchestration and response synthesis.

The system is designed to handle complex queries that require:
- **Multi-step reasoning** and research planning
- **Context-aware retrieval** from knowledge graphs and vector stores
- **Mathematical calculations** and formula extraction
- **Conversation memory** and contextual follow-ups
- **Parallel research execution** for efficiency
- **High-performance caching** for instant responses

## ğŸ—ï¸ Architecture

### Core Components

1. **LangGraph Workflow Engine**: Manages state transitions and agent orchestration
2. **Specialized Agents**: Each handles specific cognitive tasks
3. **Knowledge Infrastructure**: Neo4j graph database + Redis caching
4. **Thinking Engine**: Provides transparent reasoning visibility
5. **API Layer**: FastAPI-based streaming server

### Agent Ecosystem

- **ğŸ” Triage Agent**: Classifies queries and routes to appropriate workflows
- **ğŸ’­ Contextual Answering Agent**: Handles follow-up questions using conversation context
- **ğŸ“‹ Planning Agent**: Breaks complex queries into research sub-tasks
- **ğŸ”® Hyde Agent**: Generates hypothetical answers for better retrieval
- **ğŸ”¬ Research Orchestrator**: Executes parallel research operations
- **âœ¨ Synthesis Agent**: Combines research into coherent responses
- **ğŸ§  Memory Agent**: Updates conversation and structured memory
- **âš ï¸ Error Handler**: Manages failures and provides fallback strategies

### Data Infrastructure

- **Neo4j**: Knowledge graph for structured document storage
- **Redis**: High-performance caching and session management
- **Vector Search**: Semantic similarity retrieval
- **Web Search**: Real-time information gathering via Tavily

## ğŸš€ Features

### Advanced Capabilities

- **ğŸ”„ Stateful Conversations**: Maintains context across multiple interactions
- **ğŸ§® Mathematical Processing**: Extracts and executes formulas from documents
- **ğŸ“Š Parallel Research**: Executes multiple research queries simultaneously
- **ğŸ¯ Precision Retrieval**: Combines vector, graph, and keyword search strategies
- **ğŸ” Transparent Reasoning**: Optional thinking mode reveals agent decision-making
- **âš¡ Stream Processing**: Real-time response streaming for better UX
- **ğŸ“ˆ Quality Metrics**: Tracks retrieval relevance and synthesis quality

### Intelligent Query Handling

- Natural language understanding with intent classification
- Complex multi-document research and synthesis
- Mathematical calculations with step-by-step explanations
- Contextual follow-ups without repeating information
- Fallback strategies for challenging queries

## ğŸ“¦ Installation

### Prerequisites

- Python 3.11+
- Redis server
- Neo4j database
- API keys for: Google Gemini, Cohere, Tavily

### Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd Backend
   ```

2. **Install dependencies**
   ```bash
   poetry install
   ```

3. **Configure environment**
   Create a `.env` file:
   ```env
   GOOGLE_API_KEY=your_google_api_key
   COHERE_API_KEY=your_cohere_api_key
   TAVILY_API_KEY=your_tavily_api_key
   NEO4J_URI=bolt://localhost:7687
   NEO4J_USERNAME=neo4j
   NEO4J_PASSWORD=your_password
   REDIS_URL=redis://localhost:6379
   LANGCHAIN_API_KEY=your_langsmith_key  # Optional
   ```

4. **Initialize the database**
   ```bash
   python manage_neo4j_indexes.py
   ```

5. **Run the server**
   ```bash
   poetry run uvicorn server:app --reload
   ```

## ğŸ”§ Configuration

Key configuration options in `config.py`:

- **Model Tiers**: Configure primary (tier_1) and secondary (tier_2) LLMs
- **Cache Settings**: Redis connection and TTL configurations
- **Search Parameters**: Retrieval limits and reranking thresholds
- **Debug Mode**: Enable detailed logging and thinking visibility

## ğŸ“¡ API Usage

### Basic Query

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "user_query": "What are the load requirements for residential buildings?",
    "thread_id": "unique-session-id"
  }'
```

### Streaming Response

The API returns Server-Sent Events (SSE) for real-time updates:

```javascript
const eventSource = new EventSource('/query');
eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);
  if (data.type === 'final_response') {
    console.log('Answer:', data.content);
  }
};
```

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
python comprehensive_test_suite.py
```

Test categories include:
- Simple queries
- Complex research scenarios
- Mathematical calculations
- Contextual follow-ups
- Error handling

## ğŸ“Š Monitoring & Debugging

### LangSmith Integration

Enable tracing by setting:
```env
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_key
```

### Thinking Mode

Enable detailed reasoning visibility:
```python
ai_system = LangGraphAgenticAI(
    thinking_detail_mode=ThinkingMode.DETAILED
)
```

### Logs

- Application logs: `logs/agent_run.log`
- Test results: `test_output.log`
- Evaluation metrics: `evaluation_log.jsonl`

## ğŸ—‚ï¸ Project Structure

```
Backend/
â”œâ”€â”€ agents/              # Specialized agent implementations
â”œâ”€â”€ core/               # Core workflow and state management
â”œâ”€â”€ tools/              # Utility tools (search, retrieval, etc.)
â”œâ”€â”€ thinking_agents/    # Enhanced reasoning capabilities
â”œâ”€â”€ knowledge_graph/    # Graph database integration
â”œâ”€â”€ data/              # Static data and configurations
â”œâ”€â”€ server.py          # FastAPI application
â”œâ”€â”€ main.py            # CLI entry point
â”œâ”€â”€ config.py          # Configuration management
â””â”€â”€ prompts.py         # Centralized prompt templates
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is proprietary software. All rights reserved.

## ğŸ™ Acknowledgments

Built with:
- [LangGraph](https://github.com/langchain-ai/langgraph) for workflow orchestration
- [LangChain](https://github.com/langchain-ai/langchain) for LLM integration
- [FastAPI](https://fastapi.tiangolo.com/) for the API layer
- [Neo4j](https://neo4j.com/) for knowledge graph storage
- [Redis](https://redis.io/) for caching 